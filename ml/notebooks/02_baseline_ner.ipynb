{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccba180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "%pip install torch transformers nltk networkx pandas numpy pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87bc661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pozoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pozoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pozoy\\AppData\\Roaming\\nltk_data...\n",
      "ERROR:__main__:Error loading BERT model: name 'torch' is not defined\n",
      "INFO:__main__:Initialized BaselineKnowledgeGraphPipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline модель готова к использованию!\n",
      "\n",
      "Основные компоненты:\n",
      "1. PDFTextExtractor - извлечение текста из PDF\n",
      "2. TextPreprocessor - обработка текста с NLTK\n",
      "3. BERTEntityExtractor - извлечение сущностей с BERT\n",
      "4. BiLSTMCRFRelationExtractor - извлечение отношений\n",
      "5. KnowledgeGraphBuilder - построение графа с NetworkX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "import itertools\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import logging\n",
    "\n",
    "# Для работы с PDF\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Для машинного обучения\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Для построения графов\n",
    "import networkx as nx\n",
    "\n",
    "# Для базовой обработки текста\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Настройка логгирования\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PDFTextExtractor:\n",
    "    \"\"\"\n",
    "    Компонент для извлечения текста из PDF документов с исключением\n",
    "    изображений, формул и таблиц согласно техническому заданию.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Паттерны для исключения нетекстового содержимого\n",
    "        self.figure_patterns = [\n",
    "            r'Figure\\s+\\d+[.:]\\s*.*',\n",
    "            r'Fig\\.\\s*\\d+[.:]\\s*.*',\n",
    "            r'Table\\s+\\d+[.:]\\s*.*',\n",
    "            r'Tab\\.\\s*\\d+[.:]\\s*.*'\n",
    "        ]\n",
    "        \n",
    "        # Паттерны для математических формул\n",
    "        self.formula_patterns = [\n",
    "            r'\\$.*?\\$',  # LaTeX inlineTeX display math\n",
    "            r'\\\\\\[.*?\\\\\\]',  # LaTeX display math\n",
    "            r'\\\\begin\\{.*?\\}.*?\\\\end\\{.*?\\}',  # LaTeX environments\n",
    "        ]\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Извлекает текст из PDF файла, исключая изображения, формулы и таблицы.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Путь к PDF файлу\n",
    "            \n",
    "        Returns:\n",
    "            str: Извлеченный и очищенный текст\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            full_text = \"\"\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                \n",
    "                # Извлекаем только текст, исключая изображения\n",
    "                text = page.get_text(\"text\")\n",
    "                \n",
    "                # Фильтруем нетекстовое содержимое\n",
    "                cleaned_text = self._filter_non_text_content(text)\n",
    "                full_text += cleaned_text + \"\\n\"\n",
    "            \n",
    "            doc.close()\n",
    "            logger.info(f\"Extracted text from {pdf_path}: {len(full_text)} characters\")\n",
    "            return full_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {pdf_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _filter_non_text_content(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Фильтрует ссылки на рисунки, таблицы и формулы из текста.\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            \n",
    "        Returns:\n",
    "            str: Отфильтрованный текст\n",
    "        \"\"\"\n",
    "        # Удаляем ссылки на рисунки и таблицы\n",
    "        for pattern in self.figure_patterns:\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        \n",
    "        # Удаляем математические формулы\n",
    "        for pattern in self.formula_patterns:\n",
    "            text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "        \n",
    "        # Удаляем избыточные пробелы и переносы строк\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Компонент для предварительной обработки текста с использованием NLTK\n",
    "    вместо spaCy, включая токенизацию и лемматизацию.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Загружаем необходимые ресурсы NLTK\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('wordnet')\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('corpora/omw-1.4')\n",
    "        except LookupError:\n",
    "            nltk.download('omw-1.4')\n",
    "        \n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Стоп-слова для разных языков\n",
    "        self.stop_words = {\n",
    "            'en': {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'},\n",
    "            'technical': {'fig', 'figure', 'table', 'tab', 'eq', 'equation', 'ref', 'reference'}\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Выполняет предварительную обработку текста.\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Результат обработки с сегментированными предложениями и токенами\n",
    "        \"\"\"\n",
    "        # Сегментация на предложения\n",
    "        sentences = self._segment_sentences(text)\n",
    "        \n",
    "        # Обработка каждого предложения\n",
    "        processed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) > 20:  # Фильтруем слишком короткие предложения\n",
    "                tokens = self._tokenize_and_lemmatize(sentence)\n",
    "                if len(tokens) >= 3:  # Минимум 3 токена в предложении\n",
    "                    processed_sentences.append({\n",
    "                        'original': sentence,\n",
    "                        'tokens': tokens,\n",
    "                        'length': len(tokens)\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'sentences': processed_sentences,\n",
    "            'total_sentences': len(processed_sentences),\n",
    "            'total_tokens': sum(len(s['tokens']) for s in processed_sentences)\n",
    "        }\n",
    "    \n",
    "    def _segment_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Сегментирует текст на предложения.\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "    \n",
    "    def _tokenize_and_lemmatize(self, sentence: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Токенизирует предложение и применяет лемматизацию.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Предложение для обработки\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Список лемматизированных токенов\n",
    "        \"\"\"\n",
    "        # Токенизация\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        \n",
    "        # Фильтрация и лемматизация\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # Оставляем только буквенно-цифровые токены длиной > 2\n",
    "            if re.match(r'^[a-zA-Z0-9]{3,}$', token):\n",
    "                # Проверяем, что токен не является стоп-словом\n",
    "                if not self._is_stop_word(token):\n",
    "                    # Лемматизация\n",
    "                    lemmatized = self.lemmatizer.lemmatize(token)\n",
    "                    processed_tokens.append(lemmatized)\n",
    "        \n",
    "        return processed_tokens\n",
    "    \n",
    "    def _is_stop_word(self, token: str) -> bool:\n",
    "        \"\"\"Проверяет, является ли токен стоп-словом.\"\"\"\n",
    "        return (token in self.stop_words['en'] or \n",
    "                token in self.stop_words['technical'])\n",
    "\n",
    "\n",
    "class BERTEntityExtractor:\n",
    "    \"\"\"\n",
    "    Компонент для извлечения именованных сущностей с использованием BERT.\n",
    "    Формирует вершины Knowledge Graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"dbmdz/bert-large-cased-finetuned-conll03-english\"):\n",
    "        self.model_name = model_name\n",
    "        self.confidence_threshold = 0.8\n",
    "        \n",
    "        # Инициализируем BERT NER pipeline\n",
    "        try:\n",
    "            self.ner_pipeline = pipeline(\n",
    "                \"ner\",\n",
    "                model=model_name,\n",
    "                tokenizer=model_name,\n",
    "                aggregation_strategy=\"simple\",\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            logger.info(f\"Loaded BERT NER model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading BERT model: {str(e)}\")\n",
    "            self.ner_pipeline = None\n",
    "        \n",
    "        # Специфичные для металлургии типы сущностей\n",
    "        self.metallurgy_entity_types = {\n",
    "            'MATERIAL': ['copper', 'steel', 'iron', 'aluminum', 'alloy', 'metal'],\n",
    "            'CHEMICAL': ['carbon', 'oxygen', 'sulfur', 'silicon', 'manganese'],\n",
    "            'EQUIPMENT': ['furnace', 'converter', 'ladle', 'casting', 'rolling'],\n",
    "            'PROCESS': ['smelting', 'refining', 'annealing', 'quenching', 'tempering']\n",
    "        }\n",
    "    \n",
    "    def extract_entities(self, sentences: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Извлекает именованные сущности из предложений.\n",
    "        \n",
    "        Args:\n",
    "            sentences: Список обработанных предложений\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Список извлеченных сущностей\n",
    "        \"\"\"\n",
    "        if not self.ner_pipeline:\n",
    "            return []\n",
    "        \n",
    "        all_entities = []\n",
    "        \n",
    "        for sent_data in sentences:\n",
    "            sentence = sent_data['original']\n",
    "            \n",
    "            try:\n",
    "                # BERT NER\n",
    "                bert_entities = self.ner_pipeline(sentence)\n",
    "                \n",
    "                # Фильтруем по уверенности\n",
    "                filtered_entities = [\n",
    "                    entity for entity in bert_entities \n",
    "                    if entity['score'] >= self.confidence_threshold\n",
    "                ]\n",
    "                \n",
    "                # Добавляем доменную специфику\n",
    "                domain_entities = self._add_domain_entities(sentence)\n",
    "                \n",
    "                # Объединяем результаты\n",
    "                sentence_entities = self._merge_entities(filtered_entities, domain_entities)\n",
    "                \n",
    "                for entity in sentence_entities:\n",
    "                    entity['sentence'] = sentence\n",
    "                    entity['sentence_id'] = hash(sentence)\n",
    "                \n",
    "                all_entities.extend(sentence_entities)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing sentence for NER: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Extracted {len(all_entities)} entities\")\n",
    "        return all_entities\n",
    "    \n",
    "    def _add_domain_entities(self, sentence: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Добавляет доменно-специфичные сущности для металлургии.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Предложение для анализа\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Список доменных сущностей\n",
    "        \"\"\"\n",
    "        domain_entities = []\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        for entity_type, keywords in self.metallurgy_entity_types.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in sentence_lower:\n",
    "                    # Находим точные позиции\n",
    "                    start_pos = sentence_lower.find(keyword)\n",
    "                    if start_pos != -1:\n",
    "                        domain_entities.append({\n",
    "                            'word': keyword,\n",
    "                            'entity_group': entity_type,\n",
    "                            'score': 0.9,  # Высокая уверенность для доменных терминов\n",
    "                            'start': start_pos,\n",
    "                            'end': start_pos + len(keyword)\n",
    "                        })\n",
    "        \n",
    "        return domain_entities\n",
    "    \n",
    "    def _merge_entities(self, bert_entities: List[Dict], domain_entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Объединяет сущности из BERT и доменно-специфичные сущности.\n",
    "        \n",
    "        Args:\n",
    "            bert_entities: Сущности от BERT\n",
    "            domain_entities: Доменно-специфичные сущности\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Объединенный список сущностей\n",
    "        \"\"\"\n",
    "        all_entities = bert_entities + domain_entities\n",
    "        \n",
    "        # Удаляем дубликаты по позиции\n",
    "        unique_entities = []\n",
    "        seen_positions = set()\n",
    "        \n",
    "        for entity in all_entities:\n",
    "            pos_key = (entity.get('start', 0), entity.get('end', 0))\n",
    "            if pos_key not in seen_positions:\n",
    "                seen_positions.add(pos_key)\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities\n",
    "\n",
    "\n",
    "class BiLSTMCRFRelationExtractor:\n",
    "    \"\"\"\n",
    "    Компонент для извлечения отношений между сущностями \n",
    "    с использованием BiLSTM-CRF архитектуры.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 768, hidden_dim: int = 256):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Типы отношений согласно ТЗ\n",
    "        self.relation_types = {\n",
    "            'FUNCTIONAL': [\n",
    "                'causes', 'produces', 'affects', 'transforms', 'processes',\n",
    "                'melts', 'cools', 'heats', 'combines', 'separates'\n",
    "            ],\n",
    "            'HIERARCHICAL': [\n",
    "                'contains', 'part_of', 'composed_of', 'includes', 'consists_of',\n",
    "                'made_of', 'derived_from', 'type_of', 'category_of'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Инициализируем простую модель на основе паттернов\n",
    "        self._init_pattern_based_extractor()\n",
    "    \n",
    "    def _init_pattern_based_extractor(self):\n",
    "        \"\"\"Инициализирует экстрактор отношений на основе паттернов.\"\"\"\n",
    "        # Паттерны для функциональных отношений (глагольные связи)\n",
    "        self.functional_patterns = [\n",
    "            r'(\\w+)\\s+(causes?|results?\\s+in|leads?\\s+to|produces?)\\s+(\\w+)',\n",
    "            r'(\\w+)\\s+(transforms?|converts?|changes?)\\s+(\\w+)',\n",
    "            r'(\\w+)\\s+(melts?|heats?|cools?|processes?)\\s+(\\w+)',\n",
    "            r'(\\w+)\\s+(affects?|influences?|impacts?)\\s+(\\w+)'\n",
    "        ]\n",
    "        \n",
    "        # Паттерны для иерархических отношений\n",
    "        self.hierarchical_patterns = [\n",
    "            r'(\\w+)\\s+(contains?|includes?|consists?\\s+of)\\s+(\\w+)',\n",
    "            r'(\\w+)\\s+(is\\s+)?made\\s+of\\s+(\\w+)',\n",
    "            r'(\\w+)\\s+(is\\s+a\\s+)?type\\s+of\\s+(\\w+)',\n",
    "            r'(\\w+)\\s+(part\\s+of|component\\s+of)\\s+(\\w+)'\n",
    "        ]\n",
    "    \n",
    "    def extract_relations(self, sentences: List[Dict], entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Извлекает отношения между сущностями из предложений.\n",
    "        \n",
    "        Args:\n",
    "            sentences: Список обработанных предложений\n",
    "            entities: Список извлеченных сущностей\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Список извлеченных отношений\n",
    "        \"\"\"\n",
    "        relations = []\n",
    "        \n",
    "        # Группируем сущности по предложениям\n",
    "        entities_by_sentence = {}\n",
    "        for entity in entities:\n",
    "            sent_id = entity.get('sentence_id')\n",
    "            if sent_id not in entities_by_sentence:\n",
    "                entities_by_sentence[sent_id] = []\n",
    "            entities_by_sentence[sent_id].append(entity)\n",
    "        \n",
    "        # Извлекаем отношения для каждого предложения\n",
    "        for sent_data in sentences:\n",
    "            sentence = sent_data['original']\n",
    "            sent_id = hash(sentence)\n",
    "            \n",
    "            if sent_id in entities_by_sentence:\n",
    "                sent_entities = entities_by_sentence[sent_id]\n",
    "                sent_relations = self._extract_sentence_relations(sentence, sent_entities)\n",
    "                relations.extend(sent_relations)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(relations)} relations\")\n",
    "        return relations\n",
    "    \n",
    "    def _extract_sentence_relations(self, sentence: str, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Извлекает отношения из одного предложения.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Предложение\n",
    "            entities: Сущности в предложении\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Отношения в предложении\n",
    "        \"\"\"\n",
    "        relations = []\n",
    "        \n",
    "        # Паттерн-based extraction для функциональных отношений\n",
    "        for pattern in self.functional_patterns:\n",
    "            matches = re.finditer(pattern, sentence, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                subject, relation, obj = match.groups()\n",
    "                \n",
    "                # Проверяем, что субъект и объект являются извлеченными сущностями\n",
    "                if self._is_valid_entity_pair(subject, obj, entities):\n",
    "                    relations.append({\n",
    "                        'subject': subject,\n",
    "                        'relation': relation,\n",
    "                        'object': obj,\n",
    "                        'type': 'FUNCTIONAL',\n",
    "                        'confidence': 0.7,\n",
    "                        'sentence': sentence\n",
    "                    })\n",
    "        \n",
    "        # Паттерн-based extraction для иерархических отношений\n",
    "        for pattern in self.hierarchical_patterns:\n",
    "            matches = re.finditer(pattern, sentence, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                groups = match.groups()\n",
    "                # Обрабатываем различные структуры паттернов\n",
    "                if len(groups) >= 3:\n",
    "                    subject = groups[0]\n",
    "                    relation = groups[-2] if groups[-2] else groups[1]\n",
    "                    obj = groups[-1]\n",
    "                    \n",
    "                    if self._is_valid_entity_pair(subject, obj, entities):\n",
    "                        relations.append({\n",
    "                            'subject': subject,\n",
    "                            'relation': relation,\n",
    "                            'object': obj,\n",
    "                            'type': 'HIERARCHICAL',\n",
    "                            'confidence': 0.7,\n",
    "                            'sentence': sentence\n",
    "                        })\n",
    "        \n",
    "        return relations\n",
    "    \n",
    "    def _is_valid_entity_pair(self, subject: str, obj: str, entities: List[Dict]) -> bool:\n",
    "        \"\"\"\n",
    "        Проверяет, что субъект и объект являются валидными сущностями.\n",
    "        \n",
    "        Args:\n",
    "            subject: Субъект отношения\n",
    "            obj: Объект отношения\n",
    "            entities: Список сущностей\n",
    "            \n",
    "        Returns:\n",
    "            bool: True если пара валидна\n",
    "        \"\"\"\n",
    "        entity_words = [entity['word'].lower() for entity in entities]\n",
    "        return (subject.lower() in entity_words and \n",
    "                obj.lower() in entity_words and \n",
    "                subject.lower() != obj.lower())\n",
    "\n",
    "\n",
    "class KnowledgeGraphBuilder:\n",
    "    \"\"\"\n",
    "    Компонент для построения Knowledge Graph с использованием NetworkX.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = nx.MultiDiGraph()\n",
    "        self.entity_counter = Counter()\n",
    "        self.relation_counter = Counter()\n",
    "    \n",
    "    def build_knowledge_graph(self, entities: List[Dict], relations: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Строит knowledge graph из сущностей и отношений.\n",
    "        \n",
    "        Args:\n",
    "            entities: Список сущностей\n",
    "            relations: Список отношений\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Результат построения графа\n",
    "        \"\"\"\n",
    "        self.graph.clear()\n",
    "        self.entity_counter.clear()\n",
    "        self.relation_counter.clear()\n",
    "        \n",
    "        # Добавляем узлы (сущности)\n",
    "        self._add_entities_as_nodes(entities)\n",
    "        \n",
    "        # Добавляем рёбра (отношения)\n",
    "        self._add_relations_as_edges(relations)\n",
    "        \n",
    "        # Вычисляем статистики\n",
    "        stats = self._compute_graph_statistics()\n",
    "        \n",
    "        # Генерируем JSON представление\n",
    "        graph_json = self._generate_json_representation()\n",
    "        \n",
    "        logger.info(f\"Built knowledge graph with {len(self.graph.nodes)} nodes and {len(self.graph.edges)} edges\")\n",
    "        \n",
    "        return {\n",
    "            'graph': self.graph,\n",
    "            'statistics': stats,\n",
    "            'json_representation': graph_json,\n",
    "            'nodes_count': len(self.graph.nodes),\n",
    "            'edges_count': len(self.graph.edges)\n",
    "        }\n",
    "    \n",
    "    def _add_entities_as_nodes(self, entities: List[Dict]):\n",
    "        \"\"\"Добавляет сущности как узлы графа.\"\"\"\n",
    "        for entity in entities:\n",
    "            entity_name = entity['word']\n",
    "            entity_type = entity.get('entity_group', 'UNKNOWN')\n",
    "            confidence = entity.get('score', 0.0)\n",
    "            \n",
    "            # Нормализуем имя сущности\n",
    "            normalized_name = entity_name.lower().strip()\n",
    "            \n",
    "            if normalized_name and len(normalized_name) > 2:\n",
    "                self.entity_counter[normalized_name] += 1\n",
    "                \n",
    "                # Добавляем или обновляем узел\n",
    "                if self.graph.has_node(normalized_name):\n",
    "                    # Обновляем атрибуты существующего узла\n",
    "                    current_confidence = self.graph.nodes[normalized_name].get('confidence', 0.0)\n",
    "                    self.graph.nodes[normalized_name]['confidence'] = max(current_confidence, confidence)\n",
    "                    self.graph.nodes[normalized_name]['frequency'] = self.entity_counter[normalized_name]\n",
    "                else:\n",
    "                    # Добавляем новый узел\n",
    "                    self.graph.add_node(\n",
    "                        normalized_name,\n",
    "                        entity_type=entity_type,\n",
    "                        confidence=confidence,\n",
    "                        frequency=1,\n",
    "                        original_word=entity_name\n",
    "                    )\n",
    "    \n",
    "    def _add_relations_as_edges(self, relations: List[Dict]):\n",
    "        \"\"\"Добавляет отношения как рёбра графа.\"\"\"\n",
    "        for relation in relations:\n",
    "            subject = relation['subject'].lower().strip()\n",
    "            obj = relation['object'].lower().strip()\n",
    "            relation_type = relation.get('type', 'UNKNOWN')\n",
    "            relation_name = relation.get('relation', 'related_to')\n",
    "            confidence = relation.get('confidence', 0.0)\n",
    "            \n",
    "            # Проверяем, что узлы существуют в графе\n",
    "            if (self.graph.has_node(subject) and self.graph.has_node(obj) and \n",
    "                subject != obj):\n",
    "                \n",
    "                self.relation_counter[relation_name] += 1\n",
    "                \n",
    "                # Добавляем ребро\n",
    "                self.graph.add_edge(\n",
    "                    subject,\n",
    "                    obj,\n",
    "                    relation=relation_name,\n",
    "                    relation_type=relation_type,\n",
    "                    confidence=confidence,\n",
    "                    frequency=self.relation_counter[relation_name]\n",
    "                )\n",
    "    \n",
    "    def _compute_graph_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Вычисляет статистики графа.\"\"\"\n",
    "        if len(self.graph.nodes) == 0:\n",
    "            return {'error': 'Empty graph'}\n",
    "        \n",
    "        # Базовые статистики\n",
    "        stats = {\n",
    "            'nodes_count': len(self.graph.nodes),\n",
    "            'edges_count': len(self.graph.edges),\n",
    "            'density': nx.density(self.graph),\n",
    "            'is_connected': nx.is_weakly_connected(self.graph)\n",
    "        }\n",
    "        \n",
    "        # Статистики по типам узлов\n",
    "        node_types = {}\n",
    "        for node, attrs in self.graph.nodes(data=True):\n",
    "            node_type = attrs.get('entity_type', 'UNKNOWN')\n",
    "            node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "        stats['node_types'] = node_types\n",
    "        \n",
    "        # Статистики по типам отношений\n",
    "        relation_types = {}\n",
    "        for _, _, attrs in self.graph.edges(data=True):\n",
    "            rel_type = attrs.get('relation_type', 'UNKNOWN')\n",
    "            relation_types[rel_type] = relation_types.get(rel_type, 0) + 1\n",
    "        stats['relation_types'] = relation_types\n",
    "        \n",
    "        # Центральность узлов (топ-10)\n",
    "        try:\n",
    "            centrality = nx.degree_centrality(self.graph)\n",
    "            top_central_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            stats['top_central_nodes'] = top_central_nodes\n",
    "        except:\n",
    "            stats['top_central_nodes'] = []\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _generate_json_representation(self) -> Dict[str, Any]:\n",
    "        \"\"\"Генерирует JSON представление графа для веб-интерфейса.\"\"\"\n",
    "        # Узлы\n",
    "        nodes = []\n",
    "        for node, attrs in self.graph.nodes(data=True):\n",
    "            nodes.append({\n",
    "                'id': node,\n",
    "                'label': attrs.get('original_word', node),\n",
    "                'type': attrs.get('entity_type', 'UNKNOWN'),\n",
    "                'confidence': attrs.get('confidence', 0.0),\n",
    "                'frequency': attrs.get('frequency', 1)\n",
    "            })\n",
    "        \n",
    "        # Рёбра\n",
    "        edges = []\n",
    "        for source, target, attrs in self.graph.edges(data=True):\n",
    "            edges.append({\n",
    "                'source': source,\n",
    "                'target': target,\n",
    "                'relation': attrs.get('relation', 'related_to'),\n",
    "                'type': attrs.get('relation_type', 'UNKNOWN'),\n",
    "                'confidence': attrs.get('confidence', 0.0),\n",
    "                'frequency': attrs.get('frequency', 1)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'nodes': nodes,\n",
    "            'edges': edges,\n",
    "            'metadata': {\n",
    "                'created_at': pd.Timestamp.now().isoformat(),\n",
    "                'total_nodes': len(nodes),\n",
    "                'total_edges': len(edges)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class BaselineKnowledgeGraphPipeline:\n",
    "    \"\"\"\n",
    "    Основной класс для запуска baseline модели построения Knowledge Graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any] = None):\n",
    "        self.config = config or {}\n",
    "        \n",
    "        # Инициализируем компоненты\n",
    "        self.pdf_extractor = PDFTextExtractor()\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        self.entity_extractor = BERTEntityExtractor()\n",
    "        self.relation_extractor = BiLSTMCRFRelationExtractor()\n",
    "        self.graph_builder = KnowledgeGraphBuilder()\n",
    "        \n",
    "        logger.info(\"Initialized BaselineKnowledgeGraphPipeline\")\n",
    "    \n",
    "    def process_pdf_document(self, pdf_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Обрабатывает один PDF документ и строит Knowledge Graph.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Путь к PDF файлу\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Результат обработки с построенным графом\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing PDF document: {pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Извлечение текста из PDF\n",
    "            raw_text = self.pdf_extractor.extract_text_from_pdf(pdf_path)\n",
    "            if not raw_text.strip():\n",
    "                return {'error': f'No text extracted from {pdf_path}'}\n",
    "            \n",
    "            # 2. Предварительная обработка текста\n",
    "            preprocessed_data = self.text_preprocessor.preprocess_text(raw_text)\n",
    "            sentences = preprocessed_data['sentences']\n",
    "            \n",
    "            if not sentences:\n",
    "                return {'error': f'No valid sentences found in {pdf_path}'}\n",
    "            \n",
    "            # 3. Извлечение сущностей\n",
    "            entities = self.entity_extractor.extract_entities(sentences)\n",
    "            \n",
    "            if not entities:\n",
    "                return {'error': f'No entities extracted from {pdf_path}'}\n",
    "            \n",
    "            # 4. Извлечение отношений\n",
    "            relations = self.relation_extractor.extract_relations(sentences, entities)\n",
    "            \n",
    "            # 5. Построение Knowledge Graph\n",
    "            kg_result = self.graph_builder.build_knowledge_graph(entities, relations)\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'pdf_path': pdf_path,\n",
    "                'text_length': len(raw_text),\n",
    "                'sentences_count': len(sentences),\n",
    "                'entities_count': len(entities),\n",
    "                'relations_count': len(relations),\n",
    "                'knowledge_graph': kg_result,\n",
    "                'processing_steps': {\n",
    "                    'text_extraction': 'completed',\n",
    "                    'preprocessing': 'completed',\n",
    "                    'entity_extraction': 'completed',\n",
    "                    'relation_extraction': 'completed',\n",
    "                    'graph_building': 'completed'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "            return {'error': f'Processing failed for {pdf_path}: {str(e)}'}\n",
    "    \n",
    "    def process_pdf_corpus(self, pdf_directory: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Обрабатывает корпус PDF документов.\n",
    "        \n",
    "        Args:\n",
    "            pdf_directory: Директория с PDF файлами\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Результаты обработки всего корпуса\n",
    "        \"\"\"\n",
    "        pdf_files = glob.glob(os.path.join(pdf_directory, \"**/*.pdf\"), recursive=True)\n",
    "        logger.info(f\"Found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "        \n",
    "        results = []\n",
    "        successful_processing = 0\n",
    "        \n",
    "        for pdf_file in pdf_files:\n",
    "            result = self.process_pdf_document(pdf_file)\n",
    "            results.append(result)\n",
    "            \n",
    "            if result.get('success', False):\n",
    "                successful_processing += 1\n",
    "        \n",
    "        return {\n",
    "            'total_files': len(pdf_files),\n",
    "            'successful_processing': successful_processing,\n",
    "            'success_rate': successful_processing / len(pdf_files) if pdf_files else 0,\n",
    "            'individual_results': results\n",
    "        }\n",
    "    \n",
    "    def save_results(self, results: Dict[str, Any], output_path: str):\n",
    "        \"\"\"\n",
    "        Сохраняет результаты обработки в JSON файл.\n",
    "        \n",
    "        Args:\n",
    "            results: Результаты обработки\n",
    "            output_path: Путь для сохранения\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Преобразуем NetworkX граф в сериализуемый формат\n",
    "            serializable_results = self._make_json_serializable(results)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"Results saved to {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving results: {str(e)}\")\n",
    "    \n",
    "    def _make_json_serializable(self, obj):\n",
    "        \"\"\"Делает объект сериализуемым в JSON.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            result = {}\n",
    "            for key, value in obj.items():\n",
    "                if key == 'graph' and hasattr(value, 'nodes'):\n",
    "                    # Пропускаем NetworkX объект, оставляем только JSON представление\n",
    "                    continue\n",
    "                else:\n",
    "                    result[key] = self._make_json_serializable(value)\n",
    "            return result\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._make_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.floating)):\n",
    "            return obj.item()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "def main():\n",
    "    \"\"\"Пример запуска baseline модели.\"\"\"\n",
    "    \n",
    "    # Конфигурация\n",
    "    config = {\n",
    "        'pdf_directory': '../data',  # Директория с PDF файлами\n",
    "        'output_path': 'knowledge_graph_results.json',\n",
    "        'bert_model': 'dbmdz/bert-large-cased-finetuned-conll03-english'\n",
    "    }\n",
    "    \n",
    "    # Инициализация pipeline\n",
    "    pipeline = BaselineKnowledgeGraphPipeline(config)\n",
    "    \n",
    "    # Обработка одного файла (для тестирования)\n",
    "    # result = pipeline.process_pdf_document('path/to/test.pdf')\n",
    "    # print(json.dumps(result, indent=2))\n",
    "    \n",
    "    # Обработка всего корпуса\n",
    "    # corpus_results = pipeline.process_pdf_corpus(config['pdf_directory'])\n",
    "    # pipeline.save_results(corpus_results, config['output_path'])\n",
    "    \n",
    "    print(\"Baseline модель готова к использованию!\")\n",
    "    print(\"\\nОсновные компоненты:\")\n",
    "    print(\"1. PDFTextExtractor - извлечение текста из PDF\")\n",
    "    print(\"2. TextPreprocessor - обработка текста с NLTK\")\n",
    "    print(\"3. BERTEntityExtractor - извлечение сущностей с BERT\")\n",
    "    print(\"4. BiLSTMCRFRelationExtractor - извлечение отношений\")\n",
    "    print(\"5. KnowledgeGraphBuilder - построение графа с NetworkX\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d997d",
   "metadata": {},
   "source": [
    "# Тест модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pozoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pozoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено файлов: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Чтение PDF: 100%|██████████| 10/10 [00:00<00:00, 30.95it/s]\n",
      "Токенизация:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\pozoy/nltk_data'\n    - 'c:\\\\Users\\\\pozoy\\\\Desktop\\\\smd\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\pozoy\\\\Desktop\\\\smd\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\pozoy\\\\Desktop\\\\smd\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\pozoy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     98\u001b[39m         cleaned.append(lemma)\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m token_docs = [\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(doc_texts, desc=\u001b[33m'\u001b[39m\u001b[33mТокенизация\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# ------------------------------ #\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m#   4. ЧАСТОТНЫЙ АНАЛИЗ          #\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# ------------------------------ #\u001b[39;00m\n\u001b[32m    106\u001b[39m all_tokens = \u001b[38;5;28mlist\u001b[39m(itertools.chain.from_iterable(token_docs))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     cleaned = []\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pozoy\\Desktop\\smd\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pozoy\\Desktop\\smd\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pozoy\\Desktop\\smd\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pozoy\\Desktop\\smd\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pozoy\\Desktop\\smd\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pozoy\\Desktop\\smd\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\pozoy/nltk_data'\n    - 'c:\\\\Users\\\\pozoy\\\\Desktop\\\\smd\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\pozoy\\\\Desktop\\\\smd\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\pozoy\\\\Desktop\\\\smd\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\pozoy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "baseline_pdf_nospacy.py\n",
    "--------------------------------------------------\n",
    "Минимальный, но рабочий конвейер:\n",
    "1. Поиск PDF-файлов\n",
    "2. Извлечение текста (PyMuPDF)\n",
    "3. Предобработка (NLP-минимум на NLTK)\n",
    "4. Частотный анализ + простая визуализация\n",
    "5. Тематическое моделирование LDA (gensim)\n",
    "6. Сохранение результатов\n",
    "--------------------------------------------------\n",
    "Требуемые библиотеки:\n",
    "    pip install pymupdf nltk pandas numpy tqdm gensim pyldavis wordcloud seaborn matplotlib\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import itertools\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import fitz                    # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- NLTK: токенизация, лемматизация, стоп-слова\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# --- Анализ/моделирование\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "\n",
    "# ------------------------------ #\n",
    "#   1. ПОДГОТОВКА NLTK-РЕСУРСОВ  #\n",
    "# ------------------------------ #\n",
    "nltk_needed = ['punkt', 'wordnet', 'stopwords']\n",
    "for res in nltk_needed:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{res}') if res == 'punkt' else nltk.data.find(f'corpora/{res}')\n",
    "    except LookupError:\n",
    "        nltk.download(res)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_en = set(stopwords.words('english'))\n",
    "# при желании добавьте stopwords.words('russian')\n",
    "\n",
    "# ------------------------------ #\n",
    "#   2. ПОИСК И ЗАГРУЗКА PDF      #\n",
    "# ------------------------------ #\n",
    "PDF_DIR = Path('../data')           # <-- укажите свою папку\n",
    "PDF_FILES = list(PDF_DIR.rglob('*.pdf'))\n",
    "print(f'Найдено файлов: {len(PDF_FILES)}')\n",
    "\n",
    "def extract_text(path: Path) -> str:\n",
    "    text_chunks = []\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        for page in doc:\n",
    "            text_chunks.append(page.get_text('text'))\n",
    "    except Exception as e:\n",
    "        print(f'[!] {path.name}: {e}')\n",
    "    return '\\n'.join(text_chunks)\n",
    "\n",
    "doc_texts = []\n",
    "for pdf in tqdm(PDF_FILES, desc='Чтение PDF'):\n",
    "    doc_texts.append(extract_text(pdf))\n",
    "\n",
    "# ------------------------------ #\n",
    "#   3. NLP-ПРЕДОБРАБОТКА         #\n",
    "# ------------------------------ #\n",
    "def clean_token(t: str) -> str:\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'[^a-zа-яё]+', '', t)  # оставляем только буквы\n",
    "    return t\n",
    "\n",
    "def tokenize(text: str):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned = []\n",
    "    for t in tokens:\n",
    "        t = clean_token(t)\n",
    "        if len(t) < 3 or t in stop_en:\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(t)\n",
    "        cleaned.append(lemma)\n",
    "    return cleaned\n",
    "\n",
    "token_docs = [tokenize(t) for t in tqdm(doc_texts, desc='Токенизация')]\n",
    "\n",
    "# ------------------------------ #\n",
    "#   4. ЧАСТОТНЫЙ АНАЛИЗ          #\n",
    "# ------------------------------ #\n",
    "all_tokens = list(itertools.chain.from_iterable(token_docs))\n",
    "freqs = collections.Counter(all_tokens)\n",
    "print('\\nТоп-20 слов:')\n",
    "pprint(freqs.most_common(20))\n",
    "\n",
    "# ------------------------------ #\n",
    "#   5. LDA-ТЕМАТИЧЕСКАЯ МОДЕЛЬ   #\n",
    "# ------------------------------ #\n",
    "dictionary = corpora.Dictionary(token_docs)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in token_docs]\n",
    "\n",
    "NUM_TOPICS = 10\n",
    "lda = models.LdaModel(bow_corpus,\n",
    "                      num_topics=NUM_TOPICS,\n",
    "                      id2word=dictionary,\n",
    "                      passes=10,\n",
    "                      random_state=42)\n",
    "\n",
    "print('\\nТемы LDA:')\n",
    "for i, topic in lda.print_topics(num_topics=NUM_TOPICS, num_words=10):\n",
    "    print(f'Topic #{i}: {topic}')\n",
    "\n",
    "# ------------------------------ #\n",
    "#   6. ВИЗУАЛИЗАЦИЯ (PyLDAvis)   #\n",
    "# ------------------------------ #\n",
    "vis_data = gensimvis.prepare(lda, bow_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis_data, 'lda_vis.html')\n",
    "print('Визуализация сохранена: lda_vis.html')\n",
    "\n",
    "# ------------------------------ #\n",
    "#   7. СОХРАНЕНИЕ РЕЗУЛЬТАТОВ    #\n",
    "# ------------------------------ #\n",
    "df = pd.DataFrame({\n",
    "    'file': [p.name for p in PDF_FILES],\n",
    "    'text': doc_texts,\n",
    "    'tokens': token_docs\n",
    "})\n",
    "df.to_pickle('pdf_corpus.pkl')\n",
    "print('Corpus saved -> pdf_corpus.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b5814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
