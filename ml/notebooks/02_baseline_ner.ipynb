{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ccba180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers==4.41.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (4.41.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: seqeval in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: snorkel in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from transformers==4.41.0) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from seqeval) (1.7.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: munkres>=1.0.6 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from snorkel) (1.1.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from snorkel) (1.16.0)\n",
      "Requirement already satisfied: tensorboard>=2.13.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from snorkel) (2.20.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from snorkel) (6.31.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from requests->transformers==4.41.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from requests->transformers==4.41.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from requests->transformers==4.41.0) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from tensorboard>=2.13.0->snorkel) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\knyaz_ayotgwn\\smd\\smd_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.13.0->snorkel) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ШАГ 0. Установка зависимостей (запустить ОДИН раз)\n",
    "%pip install -U torch torchvision torchaudio \\\n",
    "             transformers==4.41.0 datasets seqeval accelerate \\\n",
    "             snorkel networkx pymupdf tqdm rapidfuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87bc661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\knyaz_ayotgwn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\knyaz_ayotgwn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ШАГ 1. Системные импорты и базовые настройки\n",
    "import os, re, json, glob, logging, random, itertools\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "import fitz                           # PyMuPDF\n",
    "from rapidfuzz import fuzz, process   # быстрые строчные сопоставления\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          DataCollatorForTokenClassification, TrainingArguments,\n",
    "                          Trainer, pipeline, AutoConfig)\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from seqeval.metrics import classification_report\n",
    "import torch, nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Общая конфигурация\n",
    "class CFG:\n",
    "    pdf_dir        = \"../data\"               # папка с входными PDF\n",
    "    weak_label_dir = \"weak_labels\"           # куда сохранить промежуточную разметку\n",
    "    model_ckpt     = \"m3rg-iitd/matscibert\"  # encoder-only доменная модель\n",
    "    max_len        = 192\n",
    "    num_epochs     = 4\n",
    "    lr             = 3e-5\n",
    "    batch_size     = 8\n",
    "cfg = CFG()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d997d",
   "metadata": {},
   "source": [
    "# Тест модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71d854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Found 0 pdfs\n",
      "0it [00:00, ?it/s]\n",
      "INFO:__main__:Total sentences: 0\n"
     ]
    }
   ],
   "source": [
    "# ШАГ 2. Извлечение “чистого” текста (ENG only, без формул/рисунков)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "non_eng_pattern = re.compile(r'[А-Яа-яЁё]+')\n",
    "latex_pattern   = re.compile(r'\\$[^$]*\\$|\\\\\\[.*?\\\\\\]|\\\\begin\\{.*?}', re.S)\n",
    "\n",
    "def clean_text(page_txt:str)->str:\n",
    "    # убираем формулы, пустые строки, кириллицу\n",
    "    t = latex_pattern.sub(' ', page_txt)\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    if non_eng_pattern.search(t):           # если страница русская — пропускаем\n",
    "        return \"\"\n",
    "    return t\n",
    "\n",
    "def pdf_to_sentences(pdf_path:Path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    raw = \" \".join(clean_text(p.get_text(\"text\")) for p in doc)\n",
    "    doc.close()\n",
    "    # сегментация предложений\n",
    "    sents = [s.strip() for s in sent_tokenize(raw) if len(s.split())>3]\n",
    "    return sents\n",
    "\n",
    "pdf_files = list(Path(cfg.pdf_dir).rglob(\"*.pdf\"))\n",
    "logger.info(f\"Found {len(pdf_files)} pdfs\")\n",
    "\n",
    "sentences = []\n",
    "for f in tqdm(pdf_files):\n",
    "    sentences += pdf_to_sentences(f)\n",
    "logger.info(f\"Total sentences: {len(sentences):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119b5814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\knyaz_ayotgwn\\smd\\smd_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\knyaz_ayotgwn\\smd\\smd_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\knyaz_ayotgwn\\.cache\\huggingface\\hub\\models--m3rg-iitd--matscibert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "weak-label: 0it [00:00, ?it/s]\n",
      "INFO:__main__:Weak-labelled sentences: 0\n"
     ]
    }
   ],
   "source": [
    "# ШАГ 3. Словарь металлургических сущностей\n",
    "gazetteer = {\n",
    "    \"MATERIAL\":  [\"steel\",\"stainless steel\",\"carbon steel\",\"alloy\",\"copper\",\"aluminium\",\n",
    "                  \"nickel\",\"titanium\",\"bronze\",\"cast iron\",\"iron\",\"slag\",\"billet\",\"slab\"],\n",
    "    \"EQUIPMENT\": [\"furnace\",\"converter\",\"ladle\",\"rolling mill\",\"caster\",\"annealing line\",\n",
    "                  \"blast furnace\",\"basic oxygen furnace\",\"electric arc furnace\"],\n",
    "    \"PROCESS\":   [\"smelting\",\"rolling\",\"casting\",\"annealing\",\"forging\",\"quenching\",\n",
    "                  \"tempering\",\"pickling\",\"hot rolling\",\"cold rolling\",\"heat treatment\"],\n",
    "    \"CHEMICAL\":  [\"carbon\",\"manganese\",\"chromium\",\"silicon\",\"phosphorus\",\"sulfur\",\"vanadium\"],\n",
    "    \"STANDARD\":  [\"ASTM\",\"EN\",\"ISO\",\"DIN\",\"JIS\",\"GOST\"],\n",
    "}\n",
    "\n",
    "# быстрое обратное индексирование\n",
    "flat2type = {v.lower():k for k,vs in gazetteer.items() for v in vs}\n",
    "lex_sorted = sorted(flat2type, key=len, reverse=True)   # longest → shortest\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_ckpt)\n",
    "\n",
    "def weak_label_sentence(sent):\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    labels = [\"O\"]*len(tokens)\n",
    "\n",
    "    sent_low = sent.lower()\n",
    "    # находим все появления любого ключевого слова\n",
    "    for kw in lex_sorted:\n",
    "        for m in re.finditer(r'\\b'+re.escape(kw)+r'\\b', sent_low):\n",
    "            # перевести char span → token span\n",
    "            char_start, char_end = m.span()\n",
    "            token_spans = tokenizer(sent, return_offsets_mapping=True,\n",
    "                                    truncation=True, max_length=cfg.max_len).offset_mapping\n",
    "            toks_in_span = [i for i,(s,e) in enumerate(token_spans)\n",
    "                            if s>=char_start and e<=char_end]\n",
    "            if not toks_in_span: continue\n",
    "            ent_type = flat2type[kw]\n",
    "            labels[toks_in_span[0]] = f\"B-{ent_type}\"\n",
    "            for tid in toks_in_span[1:]:\n",
    "                labels[tid] = f\"I-{ent_type}\"\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "# генерируем разметку sentence→BIO\n",
    "weak_data = []\n",
    "for s in tqdm(sentences, desc=\"weak-label\"):\n",
    "    toks, labs = weak_label_sentence(s)\n",
    "    if \"B-\" in \" \".join(labs):              # отбрасываем пустые\n",
    "        weak_data.append({\"tokens\":toks, \"ner_tags\":labs, \"text\":s})\n",
    "\n",
    "logger.info(f\"Weak-labelled sentences: {len(weak_data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3247985e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name ['ner_tags', 'text'] not in the dataset. Current columns in the dataset: []",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m ds = ds.train_test_split(test_size=\u001b[32m0.1\u001b[39m, seed=\u001b[32m42\u001b[39m)\n\u001b[32m     11\u001b[39m ds_dict = DatasetDict({\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:ds[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m:ds[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ds_dict = \u001b[43mds_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mner_tags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# оставим tokens+labels\u001b[39;00m\n\u001b[32m     13\u001b[39m ds_dict = ds_dict.cast_column(\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m, ClassLabel(names=unique_tags))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\knyaz_ayotgwn\\smd\\smd_env\\Lib\\site-packages\\datasets\\dataset_dict.py:375\u001b[39m, in \u001b[36mDatasetDict.remove_columns\u001b[39m\u001b[34m(self, column_names)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28mself\u001b[39m._check_values_type()\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\knyaz_ayotgwn\\smd\\smd_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m self_format = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    558\u001b[39m }\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\knyaz_ayotgwn\\smd\\smd_env\\Lib\\site-packages\\datasets\\fingerprint.py:442\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\knyaz_ayotgwn\\smd\\smd_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:2233\u001b[39m, in \u001b[36mDataset.remove_columns\u001b[39m\u001b[34m(self, column_names, new_fingerprint)\u001b[39m\n\u001b[32m   2231\u001b[39m missing_columns = \u001b[38;5;28mset\u001b[39m(column_names) - \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m._data.column_names)\n\u001b[32m   2232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[32m-> \u001b[39m\u001b[32m2233\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2234\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2235\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2236\u001b[39m     )\n\u001b[32m   2238\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[32m   2239\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m dataset._info.features[column_name]\n",
      "\u001b[31mValueError\u001b[39m: Column name ['ner_tags', 'text'] not in the dataset. Current columns in the dataset: []"
     ]
    }
   ],
   "source": [
    "# ШАГ 4. В datasets + разделение train/valid\n",
    "unique_tags = sorted({t for ex in weak_data for t in ex[\"ner_tags\"]})\n",
    "tag2id = {t:i for i,t in enumerate(unique_tags)}\n",
    "\n",
    "def encode_example(ex):\n",
    "    ex[\"labels\"] = [tag2id[t] for t in ex[\"ner_tags\"]]\n",
    "    return ex\n",
    "\n",
    "ds = Dataset.from_list(weak_data).map(encode_example, remove_columns=[])\n",
    "ds = ds.train_test_split(test_size=0.1, seed=42)\n",
    "ds_dict = DatasetDict({\"train\":ds[\"train\"], \"validation\":ds[\"test\"]})\n",
    "ds_dict = ds_dict.remove_columns([\"ner_tags\",\"text\"])   # оставим tokens+labels\n",
    "ds_dict = ds_dict.cast_column(\"labels\", ClassLabel(names=unique_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ШАГ 5. Tokenize с выравниванием меток\n",
    "def align_labels(batch):\n",
    "    tokenized = tokenizer(batch[\"tokens\"],\n",
    "                          is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=cfg.max_len)\n",
    "    new_labels = []\n",
    "    for i, word_ids in enumerate(tokenized.word_ids(batch_index=None)):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        sent_labels = batch[\"labels\"][i]\n",
    "        label_ids = []\n",
    "        prev = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            elif wid!=prev:\n",
    "                label_ids.append(sent_labels[wid])\n",
    "            else:\n",
    "                label_ids.append(sent_labels[wid] if sent_labels[wid]!=0 else -100)\n",
    "            prev = wid\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = new_labels\n",
    "    return tokenized\n",
    "\n",
    "ds_tok = ds_dict.map(align_labels, batched=True, remove_columns=[\"tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a485669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ШАГ 6. Обучение\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = classification_report\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    true, pred = [], []\n",
    "    for p,l in zip(preds, labels):\n",
    "        true_seq, pred_seq = [], []\n",
    "        for pi, li in zip(p,l):\n",
    "            if li!=-100:\n",
    "                true_seq.append(unique_tags[li])\n",
    "                pred_seq.append(unique_tags[pi])\n",
    "        true.append(true_seq); pred.append(pred_seq)\n",
    "    report = classification_report(true,pred,output_dict=True,zero_division=0)\n",
    "    return {\"f1\": report[\"weighted avg\"][\"f1-score\"]}\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(cfg.model_ckpt,\n",
    "                                          num_labels=len(unique_tags),\n",
    "                                          id2label={i:t for i,t in enumerate(unique_tags)},\n",
    "                                          label2id=tag2id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(cfg.model_ckpt,\n",
    "                                                        config=model_config)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ner_matsci_checkpoint\",\n",
    "    learning_rate=cfg.lr,\n",
    "    per_device_train_batch_size=cfg.batch_size,\n",
    "    per_device_eval_batch_size=cfg.batch_size,\n",
    "    num_train_epochs=cfg.num_epochs,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=ds_tok[\"train\"],\n",
    "                  eval_dataset=ds_tok[\"validation\"],\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"ner_matsci_final\")\n",
    "tokenizer.save_pretrained(\"ner_matsci_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ШАГ 7. Пайплайн NER с нашей дообученной моделью\n",
    "ner_pipe = pipeline(\"ner\",\n",
    "                    model=\"ner_matsci_final\",\n",
    "                    tokenizer=\"ner_matsci_final\",\n",
    "                    aggregation_strategy=\"simple\",\n",
    "                    device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def extract_entities_matsci(sentences):\n",
    "    entities = []\n",
    "    for sent in sentences:\n",
    "        res = ner_pipe(sent[\"original\"])\n",
    "        for ent in res:\n",
    "            ent[\"sentence_id\"] = hash(sent[\"original\"])\n",
    "            ent[\"sentence\"]    = sent[\"original\"]\n",
    "            entities.append(ent)\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEntityExtractor:\n",
    "    def __init__(self, model_dir=\"ner_matsci_final\"):\n",
    "        self.pipe = pipeline(\"ner\",\n",
    "                             model=model_dir,\n",
    "                             tokenizer=model_dir,\n",
    "                             aggregation_strategy=\"simple\",\n",
    "                             device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "    def extract_entities(self, sentences):\n",
    "        entities=[]\n",
    "        for sent in sentences:\n",
    "            try:\n",
    "                res=self.pipe(sent[\"original\"])\n",
    "                for e in res:\n",
    "                    e[\"sentence\"]=sent[\"original\"]\n",
    "                    e[\"sentence_id\"]=hash(sent[\"original\"])\n",
    "                entities.extend(res)\n",
    "            except Exception as ex:\n",
    "                logger.warning(f\"NER failed: {ex}\")\n",
    "        return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ecd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = BaselineKnowledgeGraphPipeline()\n",
    "result   = pipeline.process_pdf_corpus(cfg.pdf_dir)\n",
    "json.dump(result, open(\"kg_results.json\",\"w\"), indent=2)\n",
    "print(\"Done! KG JSON saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
