{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Полноценный бейзлайн для выделения металлургических сущностей и построения Knowledge Graph\n",
        "\n",
        "Этот ноутбук реализует сквозной пайплайн:\n",
        "1. Извлечение текста из PDF (только английский, без формул/изображений).\n",
        "2. Автоматическая слабая разметка (без ручной аннотации).\n",
        "3. Fine-tuning MatSciBERT для NER.\n",
        "4. Извлечение функциональных и иерархических связей.\n",
        "5. Построение и экспорт Knowledge Graph в JSON.\n",
        "\n",
        "Все шаги основаны на предыдущих обсуждениях в чате."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 0. Установка зависимостей (запустить ОДИН раз)\n",
        "!pip install -U torch torchvision torchaudio \\\n",
        "             transformers==4.41.0 datasets seqeval accelerate \\\n",
        "             snorkel networkx pymupdf tqdm rapidfuzz nltk openai py2neo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 1. Системные импорты и базовые настройки\n",
        "import os, re, json, glob, logging, random, itertools\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import multiprocessing as mp\n",
        "\n",
        "import fitz                           # PyMuPDF\n",
        "from rapidfuzz import fuzz, process   # быстрые строчные сопоставления\n",
        "\n",
        "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
        "                          DataCollatorForTokenClassification, TrainingArguments,\n",
        "                          Trainer, pipeline, AutoConfig)\n",
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "from seqeval.metrics import classification_report\n",
        "import torch, nltk\n",
        "import openai\n",
        "from snorkel.labeling import labeling_function, LabelModel, PandasLFApplier\n",
        "import networkx as nx\n",
        "from py2neo import Graph, Node, Relationship\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Общая конфигурация\n",
        "class CFG:\n",
        "    pdf_dir        = \"./data\"               # папка с входными PDF\n",
        "    weak_label_dir = \"weak_labels\"           # куда сохранить промежуточную разметку\n",
        "    model_ckpt     = \"m3rg-iitd/matscibert\"  # encoder-only доменная модель\n",
        "    max_len        = 192\n",
        "    num_epochs     = 4\n",
        "    lr             = 3e-5\n",
        "    batch_size     = 8\n",
        "    openai_key     = os.getenv(\"OPENAI_API_KEY\")  # укажите ваш ключ\n",
        "cfg = CFG()\n",
        "openai.api_key = cfg.openai_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 2. Извлечение “чистого” текста (ENG only, без формул/рисунков)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "non_eng_pattern = re.compile(r'[А-Яа-яЁё]+')\n",
        "latex_pattern = re.compile(r'\\$[^$]*\\$|\\\\$.*?\\\\$|\\\\begin\\{.*?\\}', re.S)\n",
        "\n",
        "def clean_text(page_txt:str)->str:\n",
        "    # убираем формулы, пустые строки, кириллицу\n",
        "    t = latex_pattern.sub(' ', page_txt)\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    if non_eng_pattern.search(t):           # если страница русская — пропускаем\n",
        "        return \"\"\n",
        "    return t\n",
        "\n",
        "def pdf_to_sentences(pdf_path:Path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    raw = \" \".join(clean_text(p.get_text(\"text\")) for p in doc)\n",
        "    doc.close()\n",
        "    # сегментация предложений\n",
        "    sents = [s.strip() for s in sent_tokenize(raw) if len(s.split())>3]\n",
        "    return sents\n",
        "\n",
        "pdf_files = list(Path(cfg.pdf_dir).rglob(\"*.pdf\"))\n",
        "logger.info(f\"Found {len(pdf_files)} pdfs\")\n",
        "\n",
        "sentences = []\n",
        "for f in tqdm(pdf_files):\n",
        "    sentences += pdf_to_sentences(f)\n",
        "logger.info(f\"Total sentences: {len(sentences):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 3. Словарь металлургических сущностей и labeling functions\n",
        "gazetteer = {\n",
        "    \"MATERIAL\":  [\"steel\",\"stainless steel\",\"carbon steel\",\"alloy\",\"copper\",\"aluminium\",\n",
        "                  \"nickel\",\"titanium\",\"bronze\",\"cast iron\",\"iron\",\"slag\",\"billet\",\"slab\"],\n",
        "    \"EQUIPMENT\": [\"furnace\",\"converter\",\"ladle\",\"rolling mill\",\"caster\",\"annealing line\",\n",
        "                  \"blast furnace\",\"basic oxygen furnace\",\"electric arc furnace\"],\n",
        "    \"PROCESS\":   [\"smelting\",\"rolling\",\"casting\",\"annealing\",\"forging\",\"quenching\",\n",
        "                  \"tempering\",\"pickling\",\"hot rolling\",\"cold rolling\",\"heat treatment\"],\n",
        "    \"CHEMICAL\":  [\"carbon\",\"manganese\",\"chromium\",\"silicon\",\"phosphorus\",\"sulfur\",\"vanadium\"],\n",
        "    \"STANDARD\":  [\"ASTM\",\"EN\",\"ISO\",\"DIN\",\"JIS\",\"GOST\"],\n",
        "}\n",
        "\n",
        "# быстрое обратное индексирование\n",
        "flat2type = {v.lower():k for k,vs in gazetteer.items() for v in vs}\n",
        "lex_sorted = sorted(flat2type, key=len, reverse=True)   # longest → shortest\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_ckpt)\n",
        "\n",
        "# Определение меток\n",
        "ABSTAIN = -1\n",
        "MATERIAL, EQUIPMENT, PROCESS, CHEMICAL, STANDARD = 0,1,2,3,4\n",
        "\n",
        "@labeling_function()\n",
        "def lf_gazetteer(x):\n",
        "    kw = x.token.lower()\n",
        "    if kw in flat2type:\n",
        "        return {'MATERIAL':0, 'EQUIPMENT':1, 'PROCESS':2, 'CHEMICAL':3, 'STANDARD':4}[flat2type[kw]]\n",
        "    return ABSTAIN\n",
        "\n",
        "@labeling_function()\n",
        "def lf_llm(x):\n",
        "    prompt = f\"Classify token '{x.token}' in sentence '{x.sent}' as one of: MATERIAL, EQUIPMENT, PROCESS, CHEMICAL, STANDARD or O. Return only the label.\"\n",
        "    rsp = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o-mini\", temperature=0.0,\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}]\n",
        "    )\n",
        "    label = rsp.choices.message.content.strip()\n",
        "    if label in ['MATERIAL', 'EQUIPMENT', 'PROCESS', 'CHEMICAL', 'STANDARD']:\n",
        "        return {'MATERIAL':0, 'EQUIPMENT':1, 'PROCESS':2, 'CHEMICAL':3, 'STANDARD':4}[label]\n",
        "    return ABSTAIN\n",
        "\n",
        "lfs = [lf_gazetteer, lf_llm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 4. Применение Snorkel и генерация серебряных меток\n",
        "def prepare_df(sentences):\n",
        "    data = []\n",
        "    for sent in sentences:\n",
        "        tokens = word_tokenize(sent)\n",
        "        for tok in tokens:\n",
        "            data.append({'sent': sent, 'token': tok})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "df = prepare_df(sentences)\n",
        "applier = PandasLFApplier(lfs=lfs)\n",
        "L = applier.apply(df)\n",
        "label_model = LabelModel(cardinality=5, verbose=True)\n",
        "label_model.fit(L_train=L, n_epochs=500, log_freq=50, seed=42)\n",
        "preds = label_model.predict(L)\n",
        "df['label'] = preds\n",
        "df = df[df['label'] != ABSTAIN]  # отбрасываем неопределённые"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 5. Преобразование в BIO-формат и Dataset\n",
        "unique_tags = ['O', 'B-MATERIAL', 'I-MATERIAL', 'B-EQUIPMENT', 'I-EQUIPMENT',\n",
        "               'B-PROCESS', 'I-PROCESS', 'B-CHEMICAL', 'I-CHEMICAL',\n",
        "               'B-STANDARD', 'I-STANDARD']\n",
        "tag2id = {t:i for i,t in enumerate(unique_tags)}\n",
        "\n",
        "def group_to_bio(group):\n",
        "    tokens = group['token'].tolist()\n",
        "    labels = []\n",
        "    for i, lbl in enumerate(group['label']):\n",
        "        tag = unique_tags[2 * lbl + 1] if i == 0 else unique_tags[2 * lbl + 2]  # B- / I-\n",
        "        labels.append(tag)\n",
        "    return {'tokens': tokens, 'ner_tags': labels}\n",
        "\n",
        "grouped = df.groupby('sent').apply(group_to_bio).tolist()\n",
        "ds = Dataset.from_list(grouped)\n",
        "ds = ds.train_test_split(test_size=0.1, seed=42)\n",
        "ds_dict = DatasetDict({\"train\": ds[\"train\"], \"validation\": ds[\"test\"]})\n",
        "ds_dict = ds_dict.cast_column(\"ner_tags\", ClassLabel(names=unique_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 6. Токенизация + выравнивание BIO\n",
        "def align_labels(batch):\n",
        "    tokenized = tokenizer(batch[\"tokens\"],\n",
        "                          is_split_into_words=True,\n",
        "                          truncation=True,\n",
        "                          padding=\"max_length\",\n",
        "                          max_length=cfg.max_len)\n",
        "    new_labels = []\n",
        "    for i in range(len(batch[\"ner_tags\"])):\n",
        "        word_ids = tokenized.word_ids(batch_index=i)\n",
        "        sent_labels = [tag2id[t] for t in batch[\"ner_tags\"][i]]\n",
        "        label_ids = []\n",
        "        prev = None\n",
        "        for wid in word_ids:\n",
        "            if wid is None:\n",
        "                label_ids.append(-100)\n",
        "            elif wid != prev:\n",
        "                label_ids.append(sent_labels[wid])\n",
        "            else:\n",
        "                label_ids.append(sent_labels[wid] if sent_labels[wid] % 2 == 1 else sent_labels[wid] + 1)  # B to I\n",
        "            prev = wid\n",
        "        new_labels.append(label_ids)\n",
        "    tokenized[\"labels\"] = new_labels\n",
        "    return tokenized\n",
        "\n",
        "ds_tok = ds_dict.map(align_labels, batched=True, remove_columns=ds_dict[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 7. Fine-tuning MatSciBERT\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    true, pred = [], []\n",
        "    for p, l in zip(preds, labels):\n",
        "        true_seq, pred_seq = [], []\n",
        "        for pi, li in zip(p, l):\n",
        "            if li != -100:\n",
        "                true_seq.append(unique_tags[li])\n",
        "                pred_seq.append(unique_tags[pi])\n",
        "        true.append(true_seq)\n",
        "        pred.append(pred_seq)\n",
        "    report = classification_report(true, pred, output_dict=True, zero_division=0)\n",
        "    return {\"f1\": report[\"weighted avg\"][\"f1-score\"]}\n",
        "\n",
        "model_config = AutoConfig.from_pretrained(cfg.model_ckpt,\n",
        "                                          num_labels=len(unique_tags),\n",
        "                                          id2label={i: t for i, t in enumerate(unique_tags)},\n",
        "                                          label2id=tag2id)\n",
        "model = AutoModelForTokenClassification.from_pretrained(cfg.model_ckpt, config=model_config)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"ner_matsci_checkpoint\",\n",
        "    learning_rate=cfg.lr,\n",
        "    per_device_train_batch_size=cfg.batch_size,\n",
        "    per_device_eval_batch_size=cfg.batch_size,\n",
        "    num_train_epochs=cfg.num_epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  args=args,\n",
        "                  train_dataset=ds_tok[\"train\"],\n",
        "                  eval_dataset=ds_tok[\"validation\"],\n",
        "                  data_collator=data_collator,\n",
        "                  tokenizer=tokenizer,\n",
        "                  compute_metrics=compute_metrics)\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"ner_matsci_final\")\n",
        "tokenizer.save_pretrained(\"ner_matsci_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 8. Инференс NER\n",
        "ner_pipe = pipeline(\"ner\",\n",
        "                    model=\"ner_matsci_final\",\n",
        "                    tokenizer=\"ner_matsci_final\",\n",
        "                    aggregation_strategy=\"simple\",\n",
        "                    device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "def extract_entities(sentences):\n",
        "    entities = []\n",
        "    for sent in tqdm(sentences):\n",
        "        res = ner_pipe(sent)\n",
        "        for ent in res:\n",
        "            ent['sentence'] = sent\n",
        "            ent['sentence_id'] = hash(sent)\n",
        "        entities.extend(res)\n",
        "    return entities\n",
        "\n",
        "all_entities = extract_entities(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 9. Извлечение связей (функциональные и иерархические)\n",
        "def extract_functional_relations(entities, sentences):\n",
        "    relations = []\n",
        "    for sent in sentences:\n",
        "        sent_ents = [e for e in entities if e['sentence'] == sent]\n",
        "        if len(sent_ents)  'steel')\n",
        "    for e1 in entities:\n",
        "        for e2 in entities:\n",
        "            if e1 != e2 and e2['word'] in e1['word'] and e1['entity_group'] == e2['entity_group']:\n",
        "                relations.append({\n",
        "                    'head': e1['word'],\n",
        "                    'relation': 'HIERARCHICAL',\n",
        "                    'tail': e2['word'],\n",
        "                    'conf': 0.7\n",
        "                })\n",
        "    return relations\n",
        "\n",
        "func_rels = extract_functional_relations(all_entities, sentences)\n",
        "hier_rels = extract_hierarchical_relations(all_entities)\n",
        "all_rels = func_rels + hier_rels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ШАГ 10. Построение Knowledge Graph и экспорт\n",
        "def build_kg(entities, relations):\n",
        "    G = nx.DiGraph()\n",
        "    for ent in entities:\n",
        "        G.add_node(ent['word'], type=ent['entity_group'], conf=ent.get('score', 1.0))\n",
        "    for rel in relations:\n",
        "        G.add_edge(rel['head'], rel['tail'], relation=rel['relation'], conf=rel['conf'])\n",
        "    return G\n",
        "\n",
        "kg = build_kg(all_entities, all_rels)\n",
        "\n",
        "# Экспорт в JSON\n",
        "kg_data = nx.readwrite.json_graph.node_link_data(kg)\n",
        "with open('knowledge_graph.json', 'w') as f:\n",
        "    json.dump(kg_data, f, indent=2)\n",
        "\n",
        "# Опционально: загрузка в Neo4j\n",
        "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
        "# for node, data in kg.nodes(data=True):\n",
        "#     neo_node = Node(data['type'], name=node, conf=data['conf'])\n",
        "#     graph.create(neo_node)\n",
        "# for u, v, data in kg.edges(data=True):\n",
        "#     rel = Relationship(kg.nodes[u], data['relation'], kg.nodes[v], conf=data['conf'])\n",
        "#     graph.create(rel)\n",
        "\n",
        "print(\"Knowledge Graph построен и сохранён в knowledge_graph.json\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}