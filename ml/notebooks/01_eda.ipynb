{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70845c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn tqdm gensim pyldavis pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0588b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Чтение PDF и извлечение текста\n",
    "pdf_dir = '../data'\n",
    "\n",
    "pdf_files = glob.glob(os.path.join(pdf_dir, '**/*.pdf'), recursive=True)\n",
    "print(f'Найдено PDF файлов: {len(pdf_files)}')\n",
    "\n",
    "def extract_text(path):\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        return '\\n'.join(page.get_text('text') for page in doc)\n",
    "    except Exception as e:\n",
    "        print(f'Ошибка чтения {path}: {e}')\n",
    "        return ''\n",
    "\n",
    "texts = [extract_text(f) for f in tqdm(pdf_files, desc='Извлекаем текст из PDF')]\n",
    "df = pd.DataFrame({'file': pdf_files, 'text': texts})\n",
    "df['char_len'] = df['text'].str.len()\n",
    "df['word_len'] = df['text'].str.split().str.len()\n",
    "print(df[['file', 'char_len', 'word_len']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Очистка и предварительная обработка текста\n",
    "stop_en = set([\n",
    "    'and', 'the', 'for', 'with', 'that', 'this', 'from', 'are', 'was', 'not',\n",
    "    'but', 'all', 'table', 'figure', 'fig', 'mm', 'wt', 'pct'\n",
    "])\n",
    "stop_ru = set([\n",
    "    'и', 'в', 'во', 'на', 'с', 'к', 'за', 'от', 'по', 'как', 'но', 'то', 'же',\n",
    "    'для', 'рис', 'табл', 'мм', 'г', 'мкм'\n",
    "])\n",
    "other_noise = {'угс', 'jni', 'ppm'}\n",
    "\n",
    "stopwords = stop_en | stop_ru | other_noise\n",
    "\n",
    "def tokenize(text):\n",
    "    # Нижний регистр, слова длиной 3+ буквы на латинице или кириллице\n",
    "    tokens = re.findall(r'[a-zа-яё\\-]{3,}', text.lower())\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "df['tokens'] = df['text'].progress_apply(tokenize)\n",
    "print(df[['file', 'tokens']].head())\n",
    "all_tokens = list(itertools.chain.from_iterable(df['tokens']))\n",
    "tok_freq = collections.Counter(all_tokens)\n",
    "print(f\"Всего уникальных токенов: {len(tok_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cd32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Частотный анализ\n",
    "top20 = tok_freq.most_common(20)\n",
    "print('Топ-20 слов по частотам:')\n",
    "for word, count in top20:\n",
    "    print(f'{word}: {count}')\n",
    "\n",
    "top20_df = pd.DataFrame(top20, columns=['token', 'freq'])\n",
    "# Удаляем строки с некорректными частотами\n",
    "top20_df['freq'] = pd.to_numeric(top20_df['freq'], errors='coerce')\n",
    "top20_df = top20_df.dropna(subset=['freq'])\n",
    "\n",
    "if top20_df.empty:\n",
    "    print('Нет данных для построения топ-20 токенов (частотный словарь пуст или данные некорректны).')\n",
    "else:\n",
    "    top20_df.plot.bar(\n",
    "        x='token', y='freq', figsize=(12, 5), title='Топ-20 слов',\n",
    "        legend=False\n",
    "    )\n",
    "    plt.xlabel('Термин')\n",
    "    plt.ylabel('Частота')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Облако слов\n",
    "if tok_freq:\n",
    "    wc = WordCloud(\n",
    "        width=900, height=700,\n",
    "        background_color='white',\n",
    "        colormap='magma'\n",
    "    ).generate_from_frequencies(tok_freq)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Облако слов корпуса металлургии')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Облако слов не построено: словарь частот пуст.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Распределение длины файлов\n",
    "if not df.empty:\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    sns.histplot(df['char_len'], bins=30, kde=True)\n",
    "    plt.title('Распределение длины текстов (символы)')\n",
    "    plt.xlabel('Число символов')\n",
    "    plt.ylabel('Количество PDF')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    sns.histplot(df['word_len'], bins=30, kde=True)\n",
    "    plt.title('Распределение длины текстов (слова)')\n",
    "    plt.xlabel('Число слов')\n",
    "    plt.ylabel('Количество PDF')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Нет данных о длине файлов — DataFrame пуст.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Доменные термины (пример)\n",
    "domain_terms = [\n",
    "    'copper', 'slag', 'alloy', 'smelting', 'molten', 'metal', 'casting', 'flux',\n",
    "    'blast furnace', 'rolling', 'coil', 'foundry', 'converter', 'sinter', 'refining',\n",
    "    'ore', 'steel', 'temperature', 'oxidation', 'impurity', 'electrolysis'\n",
    "]\n",
    "\n",
    "domain_counts = {term: tok_freq.get(term, 0) for term in domain_terms}\n",
    "print('Частоты доменных терминов в корпусе:')\n",
    "for term, count in domain_counts.items():\n",
    "    print(f'{term}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Сохранение результата\n",
    "df.to_csv('pdf_metallurgy_corpus_simple.csv', index=False)\n",
    "print('Результаты анализа сохранены в: pdf_metallurgy_corpus_simple.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
